{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch2Onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import onnxruntime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from pytocr.modeling.architectures import build_model\n",
    "from pytocr.utils.save_load import load_pretrained_params\n",
    "from utils import load_config, draw_det_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config_path = \".../PyTorchOCR/configs/det/det_r18_db.yml\"\n",
    "model_path = \".../PyTorchOCR/models/torch/db_r18.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(config_path)\n",
    "config[\"Global\"][\"distributed\"] = False\n",
    "\n",
    "# build model\n",
    "model = build_model(config[\"Architecture\"])\n",
    "# check if set use_gpu=True in paddlepaddle cpu version\n",
    "use_gpu = config[\"Global\"][\"use_gpu\"] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model = load_pretrained_params(model, model_path)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to onnx model\n",
    "input_img = torch.ones(1, 3, 736, 736)\n",
    "input_img = input_img.to(device)\n",
    "out_onnx_path = \".../models/onnx/db_r18_op10.onnx\"\n",
    "out_onnx_sim_path = \".../models/onnx/db_r18_op10_sim.onnx\"\n",
    "input_name = \"ocr_det_input\"     # 输入结点的名称\n",
    "output_name = \"ocr_det_output\"   # 输出结点的名称\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        input_img, \n",
    "        out_onnx_path, \n",
    "            verbose=False,      # 是否输出log\n",
    "        input_names=[input_name], \n",
    "        output_names=[output_name], \n",
    "        dynamic_axes= {\n",
    "            input_name: {0:'batch_size', 2:'in_width', 3:'in_height'},\n",
    "            output_name: {0:'batch_size', 2:'out_width', 3:'out_height'}}, # 动态batch+宽高\n",
    "        opset_version=10)  # 有问题就改 opset_version\n",
    "    \n",
    "# simplify  合并/删除冗余结点 转换支持操作符\n",
    "input_shapes = {input_name: list(input_img.shape)}   # 动态输入需要\n",
    "\n",
    "# use onnxsimplify to reduce reduent model.\n",
    "onnx_model = onnx.load(out_onnx_path)\n",
    "model_simp, check = simplify(\n",
    "    onnx_model, \n",
    "    dynamic_input_shape=True, \n",
    "    input_shapes=input_shapes)\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, out_onnx_sim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose test image\n",
    "img_path = r\".../test_img.png\"\n",
    "img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "resized_img = cv2.resize(img, (960, 736))\n",
    "image_data = np.array(resized_img, dtype='float32')\n",
    "image_data /= 255.\n",
    "image_data = np.transpose(image_data, (2, 0, 1))  # C H W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch infer\n",
    "torch_input = torch.from_numpy(image_data).unsqueeze(0)  # N C H W\n",
    "\n",
    "st_time = time.time()\n",
    "with torch.no_grad():\n",
    "    torch_input = torch_input.to(device)\n",
    "    print(torch_input.shape, torch_input.dtype)\n",
    "    torch_preds = model(torch_input)[\"maps\"].cpu().numpy()\n",
    "print(\"torch infer cost time\", time.time() - st_time)\n",
    "print(torch_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load onnx model\n",
    "onnx_path = out_onnx_sim_path\n",
    "session = onnxruntime.InferenceSession(onnx_path)\n",
    "session.get_modelmeta()\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "input_name, output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onnx infer\n",
    "onnx_input = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "st_time = time.time()\n",
    "onnx_preds = session.run([output_name], {input_name: onnx_input})[0]\n",
    "print(\"onnx infer cost time\", time.time() - st_time)\n",
    "print(onnx_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算输出差异\n",
    "diff = onnx_preds - torch_preds\n",
    "print(\"difference between onnx and torch: \", max(diff.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit (virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}